{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:orange\">Topic Modeling con NLP</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuente: https://pycaret.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 ¿Qué es el procesamiento del lenguaje natural?\n",
    "\n",
    "El procesamiento del lenguaje natural (PNL) es una rama de la inteligencia artificial que se ocupa de analizar, comprender y generar los lenguajes que los humanos usan de forma natural para interactuar con las computadoras en contextos tanto escritos como hablados utilizando lenguajes humanos naturales en lugar de lenguajes informáticos. Algunos de los casos de uso común de la PNL en el aprendizaje automático son:\n",
    "\n",
    "- **Descubrimiento y modelado de temas:** Capture el significado y los temas en colecciones de texto y aplique técnicas avanzadas de modelado como Modelado de temas para agrupar documentos similares.\n",
    "- **Análisis de sentimientos:** Identificar el estado de ánimo o las opiniones subjetivas dentro de grandes cantidades de texto, incluido el sentimiento promedio y la minería de opiniones.\n",
    "- **Resumen de documentos:** Generación automática de sinopsis de grandes cuerpos de texto.\n",
    "- **Conversión de voz a texto y de texto a voz:** Transformación de comandos de voz en texto escrito y viceversa.\n",
    "- **Traducción automática:** Traducción automática de texto o voz de un idioma a otro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Descripción general del módulo de procesamiento de lenguaje natural en PyCaret\n",
    "El módulo NLP de PyCaret (`pycaret.nlp`) es un módulo de aprendizaje automático no supervisado que se puede utilizar para analizar los datos de texto creando un modelo de tema para encontrar la estructura semántica oculta en los documentos. El módulo NLP de PyCaret viene integrado con una amplia gama de técnicas de preprocesamiento de texto, que es el paso fundamental en cualquier problema de NLP. Transforma el texto sin procesar en un formato del que pueden aprender los algoritmos de aprendizaje automático.\n",
    "\n",
    "A partir de la primera versión, el módulo NLP de PyCaret solo admite el idioma inglés y proporciona varias implementaciones populares de modelos de temas desde la asignación de Dirichlet latente hasta la factorización de matrices no negativas. Tiene más de 5 algoritmos listos para usar y más de 10 gráficos para analizar el texto. El módulo NLP de PyCaret también implementa una función única `tune_model ()` que le permite ajustar los hiperparámetros de un modelo de tema para optimizar el objetivo de aprendizaje supervisado, como ʻAUC` para clasificación o `R2` para regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este tutorial usaremos datos de **Kiva Microfunds** https://www.kiva.org/. Kiva Microfunds es una organización sin fines de lucro que permite a las personas prestar dinero a empresarios y estudiantes de bajos ingresos de todo el mundo. Desde su inicio en 2005, Kiva ha financiado colectivamente millones de préstamos con una tasa de reembolso de alrededor del 98%. En Kiva, cada solicitud de préstamo incluye información demográfica tradicional sobre el prestatario, como el sexo y la ubicación, así como una historia personal. En este tutorial usaremos el texto proporcionado en la historia personal para obtener información sobre el conjunto de datos y comprender la estructura semántica oculta en el texto. El conjunto de datos contiene 6818 muestras. A continuación se muestra una breve descripción de las características:\n",
    "\n",
    "- **país:** país del prestatario\n",
    "- **en:** Historia personal del prestatario cuando solicitó el préstamo\n",
    "- **género:** género (M = masculino, F = femenino)\n",
    "- **cantidad_préstamo:** Cantidad de préstamo aprobado y desembolsado\n",
    "- **impago:** Tipo de prestamista (Prestamista = usuario personal registrado en el sitio web de Kiva, Socio = institución de microfinanzas que trabaja con Kiva para encontrar y financiar préstamos)\n",
    "- **sector:** sector del prestatario\n",
    "- **estado:** estado del préstamo (1-incumplimiento, 0-reembolsado)\n",
    "\n",
    "\n",
    "\n",
    "#### Reconocimiento del conjunto de datos:\n",
    "Kiva Microfunds https://www.kiva.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycaret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Obtención de la información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>en</th>\n",
       "      <th>gender</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>nonpayment</th>\n",
       "      <th>sector</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>\"Banco Esperanza\" is a group of 10 women looki...</td>\n",
       "      <td>F</td>\n",
       "      <td>1225</td>\n",
       "      <td>partner</td>\n",
       "      <td>Retail</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>\"Caminemos Hacia Adelante\" or \"Walking Forward...</td>\n",
       "      <td>F</td>\n",
       "      <td>1975</td>\n",
       "      <td>lender</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>\"Creciendo Por La Union\" is a group of 10 peop...</td>\n",
       "      <td>F</td>\n",
       "      <td>2175</td>\n",
       "      <td>partner</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>\"Cristo Vive\" (\"Christ lives\" is a group of 10...</td>\n",
       "      <td>F</td>\n",
       "      <td>1425</td>\n",
       "      <td>partner</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>\"Cristo Vive\" is a large group of 35 people, 2...</td>\n",
       "      <td>F</td>\n",
       "      <td>4025</td>\n",
       "      <td>partner</td>\n",
       "      <td>Food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              country                                                 en  \\\n",
       "0  Dominican Republic  \"Banco Esperanza\" is a group of 10 women looki...   \n",
       "1  Dominican Republic  \"Caminemos Hacia Adelante\" or \"Walking Forward...   \n",
       "2  Dominican Republic  \"Creciendo Por La Union\" is a group of 10 peop...   \n",
       "3  Dominican Republic  \"Cristo Vive\" (\"Christ lives\" is a group of 10...   \n",
       "4  Dominican Republic  \"Cristo Vive\" is a large group of 35 people, 2...   \n",
       "\n",
       "  gender  loan_amount nonpayment    sector  status  \n",
       "0      F         1225    partner    Retail       0  \n",
       "1      F         1975     lender  Clothing       0  \n",
       "2      F         2175    partner  Clothing       0  \n",
       "3      F         1425    partner  Clothing       0  \n",
       "4      F         4025    partner      Food       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pycaret.datasets import get_data\n",
    "data = get_data('kiva')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6818, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling the data to select only 1000 documents\n",
    "data = data.sample(1000, random_state=786).reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Configuración del entorno en PyCaret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `setup()` inicializa el entorno en pycaret y realiza varios pasos de preprocesamiento de texto que son imperativos para trabajar con problemas de PNL. setup debe ser llamado antes de ejecutar cualquier otra función en pycaret. Se necesitan dos parámetros: el marco de datos de pandas y el nombre de la columna de texto pasada como parámetro `target`. También puede pasar una `lista` que contenga texto, en cuyo caso no es necesario pasar el parámetro `objetivo`. Cuando se ejecuta la configuración, los siguientes pasos de preprocesamiento se aplican automáticamente:\n",
    "\n",
    "- **Eliminación de caracteres numéricos:** Todos los caracteres numéricos se eliminan del texto. Se reemplazan con espacios en blanco. <br/>\n",
    "<br/>\n",
    "- **Eliminación de caracteres especiales:** Todos los caracteres especiales no alfanuméricos se eliminan del texto. También se reemplazan con espacios en blanco. <br/>\n",
    "<br/>\n",
    "- **Tokenización de palabras:** La tokenización de palabras es el proceso de dividir una gran muestra de texto en palabras. Este es el requisito fundamental en las tareas de procesamiento del lenguaje natural, donde cada palabra debe capturarse por separado para su posterior análisis.  <br/>\n",
    "<br/>\n",
    "- **Eliminación de palabras vacías:** Una palabra vacía (o palabra inexistente) es una palabra que a menudo se elimina del texto porque es común y proporciona poco valor para la recuperación de información, aunque puede ser lingüísticamente significativa. Ejemplos de tales palabras en inglés son: \"the\", \"a\", \"an\", \"in\", etc.<br / >\n",
    "<br/>\n",
    "- **Extracción de bigrama:** Un bigrama es una secuencia de dos elementos adyacentes de una cadena de tokens, que suelen ser letras, sílabas o palabras. Por ejemplo: la palabra Nueva York se captura como dos palabras diferentes \"Nueva\" y \"York\" cuando se realiza la tokenización, pero si se repite las veces suficientes, Bigram Extraction representará la palabra como una, es decir, \"New_York\"  <br/>\n",
    "<br/>\n",
    "\n",
    "- **Extracción de trigram:** Similar a la extracción de bigrama, el trigram es una secuencia de tres elementos adyacentes de una cadena de tokens. <br/>\n",
    "<br/>\n",
    "- **Lematización: **La lematización es el proceso de agrupar las formas flexionadas de una palabra para que puedan analizarse como una sola palabra, identificada por el lema de la palabra o la forma del diccionario. En el idioma inglés, la palabra aparece en varias formas flexionadas. Por ejemplo, el verbo \"caminar\" puede aparecer como \"caminar\", \"caminar\", \"caminar\", \"caminar\". La forma básica, 'caminar', que uno podría buscar en un diccionario, se llama lema de la palabra<br/>\n",
    "<br/>\n",
    "- **Palabras irrelevantes personalizadas:** Muchas veces el texto contiene palabras que no son palabras irrelevantes según la regla del idioma, pero que añaden poca o ninguna información. Por ejemplo, en este tutorial estamos usando el conjunto de datos de préstamos. Como tal, palabras como \"préstamo\", \"banco\", \"dinero\", \"negocio\" son demasiado obvias y no añaden valor. La mayoría de las veces, también agregan mucho ruido en el modelo de tema. Puede eliminar esas palabras del corpus utilizando el parámetro `custom_stopwords`.\n",
    "\n",
    "**Nota:** Algunas funcionalidades en `pycaret.nlp` requieren un modelo de idioma inglés. El modelo de idioma no se descarga automáticamente cuando instala pycaret. Tendrá que descargar esta interfaz de línea de comandos de Python, como Anaconda Prompt. Para descargar el modelo, escriba lo siguiente en su línea de comando:\n",
    "\n",
    "`python -m spacy descargar en_core_web_sm` <br/>\n",
    "`python -m textblob.download_corpora` <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logs:PyCaret NLP Module\n",
      "INFO:logs:version 2.1.2\n",
      "INFO:logs:Initializing setup()\n",
      "INFO:logs:USI: abfd\n",
      "INFO:logs:setup(data=(1000, 7), target=en, custom_stopwords=None, html=True, session_id=123, log_experiment=False,\n",
      "                    experiment_name=None, log_plots=False, log_data=False, verbose=True)\n",
      "INFO:logs:Checking environment\n",
      "INFO:logs:python_version: 3.7.6\n",
      "INFO:logs:python_build: ('default', 'Jan  8 2020 20:23:39')\n",
      "INFO:logs:machine: AMD64\n",
      "INFO:logs:platform: Windows-10-10.0.18362-SP0\n",
      "INFO:logs:Memory: svmem(total=17059516416, available=8020533248, percent=53.0, used=9038983168, free=8020533248)\n",
      "INFO:logs:Physical Core: 4\n",
      "INFO:logs:Logical Core: 8\n",
      "INFO:logs:Checking libraries\n",
      "INFO:logs:pd==1.1.1\n",
      "INFO:logs:numpy==1.18.1\n",
      "INFO:logs:gensim==3.8.3\n",
      "WARNING:logs:spacy not found\n",
      "INFO:logs:nltk==3.5\n",
      "INFO:logs:textblob==0.15.3\n",
      "INFO:logs:pyLDAvis==2.1.2\n",
      "INFO:logs:wordcloud==1.7.0\n",
      "INFO:logs:mlflow==1.11.0\n",
      "INFO:logs:Checking Exceptions\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Leyre\\anaconda3\\lib\\site-packages\\pycaret\\nlp.py\", line 283, in setup\n",
      "    import spacy\n",
      "  File \"C:\\Users\\Leyre\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\", line 12, in <module>\n",
      "    from . import pipeline\n",
      "  File \"C:\\Users\\Leyre\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\__init__.py\", line 4, in <module>\n",
      "    from .pipes import Tagger, DependencyParser, EntityRecognizer, EntityLinker\n",
      "  File \"pipes.pyx\", line 1, in init spacy.pipeline.pipes\n",
      "  File \"stringsource\", line 105, in init spacy.syntax.nn_parser\n",
      "AttributeError: type object 'spacy.syntax.nn_parser.array' has no attribute '__reduce_cython__'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Leyre\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-3d7c5fbfb5c1>\", line 2, in <module>\n",
      "    exp_nlp101 = setup(data = data, target = 'en', session_id = 123)\n",
      "  File \"C:\\Users\\Leyre\\anaconda3\\lib\\site-packages\\pycaret\\nlp.py\", line 286, in setup\n",
      "    sys.exit('(Type Error): spacy english model is not yet downloaded. See the documentation of setup to see installation guide.')\n",
      "SystemExit: (Type Error): spacy english model is not yet downloaded. See the documentation of setup to see installation guide.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Leyre\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Leyre\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Leyre\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Leyre\\anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pycaret\\nlp.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(data, target, custom_stopwords, html, session_id, log_experiment, experiment_name, log_plots, log_data, verbose)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m         \u001b[0msp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parser'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ner'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcli_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDependencyParser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEntityRecognizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEntityLinker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextCategorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSentencizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpipes.pyx\u001b[0m in \u001b[0;36minit spacy.pipeline.pipes\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\syntax\\nn_parser.cp37-win_amd64.pyd\u001b[0m in \u001b[0;36minit spacy.syntax.nn_parser\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'spacy.syntax.nn_parser.array' has no attribute '__reduce_cython__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-3d7c5fbfb5c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpycaret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mexp_nlp101\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pycaret\\nlp.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(data, target, custom_stopwords, html, session_id, log_experiment, experiment_name, log_plots, log_data, verbose)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'(Type Error): spacy english model is not yet downloaded. See the documentation of setup to see installation guide.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: (Type Error): spacy english model is not yet downloaded. See the documentation of setup to see installation guide.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2036\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m   2037\u001b[0m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[1;32m-> 2038\u001b[1;33m                                                                      value))\n\u001b[0m\u001b[0;32m   2039\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2040\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    821\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m--> 823\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     chained_exceptions_tb_offset, context)\n\u001b[0;32m    701\u001b[0m                 \u001b[1;33m+\u001b[0m \u001b[0mchained_exception_message\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m                 + out_list)\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1436\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1334\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1336\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m             )\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m-> 1193\u001b[1;33m                                                                tb_offset)\n\u001b[0m\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[1;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from pycaret.nlp import *\n",
    "exp_nlp101 = setup(data = data, target = 'en', session_id = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que la instalación se ejecuta con éxito, imprime la cuadrícula de información con la siguiente información:\n",
    "\n",
    "- **session_id:** Un número pseduo-aleatorio distribuido como semilla en todas las funciones para su posterior reproducibilidad. Si no se pasa `session_id`, se genera automáticamente un número aleatorio que se distribuye a todas las funciones. En este experimento, session_id se establece como `123` para su posterior reproducibilidad. <br/>\n",
    "<br/>\n",
    "- **Documentos:** Número de documentos (o muestras en el conjunto de datos si se pasa el marco de datos). <br/>\n",
    "<br/>\n",
    "- **Tamaño de vocabulario:** Tamaño del vocabulario en el corpus después de aplicar todo el preprocesamiento del texto, como la eliminación de palabras vacías, extracción de bigramas / trigramas, lematización, etc. <br/>\n",
    "\n",
    "Observe que todos los pasos de preprocesamiento de texto se realizan automáticamente cuando ejecuta `setup ()`. Estos pasos son imprescindibles para realizar cualquier experimento de PNL. La función `setup ()` prepara el corpus y el diccionario que está listo para usar para los modelos de tema que puede crear usando la función `create_model ()`. Otra forma de pasar el texto es en forma de lista, en cuyo caso no se necesita el parámetro `target`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Crear un modelo de tema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué es el modelo de tema?** En el aprendizaje automático y el procesamiento del lenguaje natural, un modelo de tema es un tipo de modelo estadístico para descubrir los \"temas\" abstractos que ocurren en una colección de documentos. El modelado de temas es una herramienta de extracción de texto de uso frecuente para el descubrimiento de estructuras semánticas ocultas en un cuerpo de texto. Intuitivamente, dado que un documento trata sobre un tema en particular, uno esperaría que aparecieran palabras específicas en el documento con más o menos frecuencia: \"perro\" y \"hueso\" aparecerán con más frecuencia en documentos sobre perros, \"gato\" y \"miau\". aparecerá en documentos sobre gatos, y \"el\" y \"es\" aparecerán igualmente en ambos. Por lo general, un documento se refiere a múltiples temas en diferentes proporciones; por lo tanto, en un documento que trata sobre gatos en un 10% y sobre perros en un 90%, probablemente habría alrededor de 9 veces más palabras de perros que de gatos. Los \"temas\" producidos por las técnicas de modelado de temas son grupos de palabras similares. Un modelo de tema captura esta intuición en un marco matemático, que permite examinar un conjunto de documentos y descubrir, con base en las estadísticas de las palabras en cada uno, cuáles podrían ser los temas y cuál es el balance de temas de cada documento. \n",
    "\n",
    "Crear un modelo de tema en PyCaret es simple y similar a cómo habría creado un modelo en módulos supervisados ​​de pycaret. Se crea un modelo de tema usando la función `create_model ()` que toma un parámetro obligatorio, es decir, el nombre del modelo como una cadena. Esta función devuelve un objeto de modelo entrenado. Hay 5 modelos de temas disponibles en PyCaret. vea la cadena de documentos de `create_model ()` para una lista completa de modelos. Vea un ejemplo a continuación donde creamos el modelo de asignación de Dirichlet latente (LDA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda = create_model('lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos creado el modelo de asignación de Dirichlet latente (LDA) con una sola palabra, es decir, `create_model ()`. Observe que el parámetro `num_topics` está establecido en` 4`, que es un valor predeterminado que se toma cuando no pasa el parámetro `num_topics` en` create_model () `. En el siguiente ejemplo, crearemos un modelo LDA con 6 temas y también estableceremos el parámetro `multi_core` en` True`. Cuando `multi_core` se establece en` True`, la asignación de Dirichlet latente (LDA) utiliza todos los núcleos de CPU para paralelizar y acelerar el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda2 = create_model('lda', num_topics = 6, multi_core = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Asignar un Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos creado un modelo de tema, nos gustaría asignar las proporciones de tema a nuestro conjunto de datos (6818 documentos / muestras) para analizar los resultados. Lo lograremos usando la función ʻassign_model ()`. Vea un ejemplo a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_results = assign_model(lda)\n",
    "lda_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe cómo ahora se agregan 6 columnas adicionales al marco de datos. ʻEn` es el texto después de todo el preprocesamiento. `Topic_0 ... Topic_3` son las proporciones de los temas y representan la distribución de temas para cada documento. `Dominant_Topic` es el número de tema con mayor proporción y` Perc_Dominant_Topic` es el porcentaje de tema dominante sobre 1 (solo se muestra cuando los modelos son estocásticos, es decir, la suma de todas las proporciones es igual a 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0 Visualizar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `plot_model ()` se puede usar para analizar el corpus general o solo temas específicos extraídos a través del modelo de temas. Por lo tanto, la función `plot_model ()` también puede funcionar sin pasar ningún objeto de modelo entrenado. Vea ejemplos a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Frequency Distribution of Entire Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Top 100 Bigrams on Entire Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(plot = 'bigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Frequency Distribution of Topic 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot_model ()` también se puede usar para analizar los mismos gráficos para temas específicos. Para generar gráficos a nivel de tema, la función requiere que el objeto de modelo entrenado se pase dentro de `plot_model ()`. En el siguiente ejemplo, generaremos una distribución de frecuencia en el `Tema 1` solo como se define en el parámetro` topic_num`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(lda, plot = 'frequency', topic_num = 'Topic 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(lda, plot = 'topic_distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada documento es una distribución de temas y no un solo tema. Aunque, si la tarea es categorizar el documento en temas específicos, no sería incorrecto usar la proporción de temas con el valor más alto para categorizar el documento en **un tema**. En el gráfico anterior, cada documento se clasifica en un tema utilizando la mayor proporción de ponderaciones de temas. Podemos ver que la mayoría de los documentos están en el \"Tema 3\" y sólo unos pocos en el \"Tema 1\". Si pasa el mouse sobre estas barras, obtendrá una idea básica de los temas de este tema al mirar las palabras clave.\n",
    "\n",
    "Por ejemplo, si evalúa el \"Tema 2\", verá palabras clave como \"agricultor\", \"arroz\", \"tierra\", lo que probablemente significa que los solicitantes de préstamos en esta categoría pertenecen a préstamos agrícolas. Sin embargo, si coloca el cursor sobre el `Tema 0` y el` Tema 3`, observará que muchas repeticiones y palabras clave se superponen en todos los temas, como la palabra \"préstamo\" y \"negocio\" que aparecen tanto en el `Tema 0` como en el` Tema 3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 T-distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(lda, plot = 'tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La incrustación de vecinos estocásticos distribuidos en T (t-SNE) es una técnica de reducción de dimensionalidad no lineal muy adecuada para incrustar datos de alta dimensión para visualización en un espacio de baja dimensión de dos o tres dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6 Uniform Manifold Approximation and Projection Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(lda, plot = 'umap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP (Uniform Manifold Approximation and Projection) es una novedosa técnica de aprendizaje múltiple para la reducción de dimensionalidad. Es similar a tSNE y PCA en su propósito, ya que todas son técnicas para reducir la dimensionalidad para proyecciones 2d / 3d. UMAP se construye a partir de un marco teórico basado en la geometría riemanniana y la topología algebraica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0 Evaluar el Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de analizar el rendimiento de los modelos es usar la función ʻevaluate_model () `que muestra una interfaz de usuario para todos los gráficos disponibles para un modelo dado. Utiliza internamente la función `plot_model ()`. Vea el ejemplo a continuación en el que hemos generado un gráfico de polaridad de sentimiento para el `Tema 3` utilizando el modelo LDA almacenado en la variable` lda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.0 Guardar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida que profundice en el procesamiento del lenguaje natural, aprenderá que el tiempo de entrenamiento de los modelos de temas aumenta exponencialmente a medida que aumenta el tamaño del corpus. Como tal, si desea continuar con su experimento o análisis en un momento posterior, no necesita repetir todo el experimento y volver a entrenar su modelo. La función incorporada de PyCaret `save_model ()` le permite guardar el modelo para su uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(lda,'Final LDA Model 08Feb2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.0 Cargar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cargar un modelo guardado en una fecha futura en el mismo entorno o en otro diferente, usaríamos la función `load_model ()` de PyCaret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_lda = load_model('Final LDA Model 08Feb2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(saved_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
